name: EKS Complete Destroy

on:
  workflow_dispatch: # Manual trigger only
    inputs:
      confirm_destroy:
        description: 'Type DESTROY to confirm'
        required: true
        default: 'NO'

env:
  AWS_REGION: us-east-1

jobs:
  destroy-everything:
    name: Complete EKS Destruction
    runs-on: ubuntu-latest
    if: github.event.inputs.confirm_destroy == 'DESTROY'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Install Helm
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Clean Applications (if cluster exists)
        run: |
          echo "=== CLEANING APPLICATIONS ==="
          
          # Check if EKS cluster exists
          if aws eks describe-cluster --name my-eks-cluster --region us-east-1 &>/dev/null; then
            echo "‚úÖ EKS cluster found, cleaning applications..."
            
            # Get EKS credentials
            aws eks update-kubeconfig --name my-eks-cluster --region us-east-1
            
            # Uninstall Helm releases
            helm uninstall myapp --wait --timeout=300s || echo "myapp not found"
            helm uninstall postgres --wait --timeout=300s || echo "postgres not found"
            helm uninstall csi-secrets-store -n kube-system --wait --timeout=300s || echo "CSI driver not found"
            
            # Delete Kubernetes resources
            kubectl delete secret db-credentials aws-ecr-secret --ignore-not-found=true --timeout=60s
            kubectl delete serviceaccount myapp-sa --ignore-not-found=true --timeout=60s
            kubectl delete secretproviderclass --all --ignore-not-found=true --timeout=60s
            
            echo "‚úÖ Applications cleaned"
          else
            echo "‚ö†Ô∏è EKS cluster not found, skipping application cleanup"
          fi

      - name: Manual AWS Resource Cleanup
        run: |
          echo "=== MANUAL AWS RESOURCE CLEANUP ==="
          
          # 1. Clean up ECR images
          echo "Cleaning ECR repository..."
          if aws ecr describe-repositories --repository-names myapp &>/dev/null; then
            aws ecr list-images --repository-name myapp --query 'imageIds[*]' --output json > /tmp/images.json
            if [ -s /tmp/images.json ] && [ "$(cat /tmp/images.json)" != "[]" ]; then
              echo "Deleting ECR images..."
              aws ecr batch-delete-image --repository-name myapp --image-ids file:///tmp/images.json || echo "Failed to delete some images"
            fi
          else
            echo "ECR repository not found"
          fi
          
          # 2. Clean up Load Balancers created by Kubernetes
          echo "Cleaning Load Balancers..."
          aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(Tags[?Key==`kubernetes.io/cluster/my-eks-cluster`].Value, `owned`)].LoadBalancerArn' --output text | while read lb_arn; do
            if [ -n "$lb_arn" ] && [ "$lb_arn" != "None" ]; then
              echo "Deleting Load Balancer: $lb_arn"
              aws elbv2 delete-load-balancer --load-balancer-arn $lb_arn || echo "Failed to delete LB"
            fi
          done
          
          # 3. Clean up Target Groups
          echo "Cleaning Target Groups..."
          aws elbv2 describe-target-groups --query 'TargetGroups[?contains(Tags[?Key==`kubernetes.io/cluster/my-eks-cluster`].Value, `owned`)].TargetGroupArn' --output text | while read tg_arn; do
            if [ -n "$tg_arn" ] && [ "$tg_arn" != "None" ]; then
              echo "Deleting Target Group: $tg_arn"
              aws elbv2 delete-target-group --target-group-arn $tg_arn || echo "Failed to delete TG"
            fi
          done
          
          # 4. Clean up Security Groups created by EKS
          echo "Cleaning Security Groups..."
          aws ec2 describe-security-groups --filters "Name=tag:kubernetes.io/cluster/my-eks-cluster,Values=owned" --query 'SecurityGroups[].GroupId' --output text | while read sg_id; do
            if [ -n "$sg_id" ] && [ "$sg_id" != "None" ]; then
              echo "Deleting Security Group: $sg_id"
              aws ec2 delete-security-group --group-id $sg_id || echo "Failed to delete SG (may have dependencies)"
            fi
          done
          
          # 5. Clean up ENIs
          echo "Cleaning ENIs..."
          aws ec2 describe-network-interfaces --filters "Name=tag:kubernetes.io/cluster/my-eks-cluster,Values=owned" --query 'NetworkInterfaces[].NetworkInterfaceId' --output text | while read eni_id; do
            if [ -n "$eni_id" ] && [ "$eni_id" != "None" ]; then
              echo "Deleting ENI: $eni_id"
              aws ec2 delete-network-interface --network-interface-id $eni_id || echo "Failed to delete ENI"
            fi
          done
          
          echo "Waiting for resources to be deleted..."
          sleep 120

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.0.0

      - name: Terraform Destroy
        run: |
          echo "=== TERRAFORM DESTROY ==="
          
          cd eks/terraform
          terraform init -upgrade
          
          # Destroy with retries
          for i in {1..3}; do
            echo "Terraform destroy attempt $i/3..."
            if terraform destroy -auto-approve 2>&1 | tee destroy_output.log; then
              echo "‚úÖ Terraform destroy completed successfully"
              break
            else
              echo "‚ùå Terraform destroy attempt $i failed"
              
              if [ $i -eq 3 ]; then
                echo "‚ùå All destroy attempts failed"
                cat destroy_output.log
                exit 1
              else
                echo "Waiting before retry..."
                sleep 180
              fi
            fi
          done

      - name: Final Verification
        run: |
          echo "=== FINAL VERIFICATION ==="
          
          # List of all resources to check
          RESOURCES_DESTROYED=0
          TOTAL_RESOURCES=7
          
          # Check EKS cluster
          if aws eks describe-cluster --name my-eks-cluster &>/dev/null; then
            echo "‚ùå EKS cluster still exists"
          else
            echo "‚úÖ EKS cluster destroyed"
            ((RESOURCES_DESTROYED++))
          fi
          
          # Check RDS instance
          if aws rds describe-db-instances --db-instance-identifier eks-postgres &>/dev/null; then
            echo "‚ùå RDS instance still exists"
          else
            echo "‚úÖ RDS instance destroyed"
            ((RESOURCES_DESTROYED++))
          fi
          
          # Check ElastiCache
          if aws elasticache describe-cache-clusters --cache-cluster-id eks-redis &>/dev/null; then
            echo "‚ùå ElastiCache cluster still exists"
          else
            echo "‚úÖ ElastiCache cluster destroyed"
            ((RESOURCES_DESTROYED++))
          fi
          
          # Check ECR repository
          if aws ecr describe-repositories --repository-names myapp &>/dev/null; then
            echo "‚ùå ECR repository still exists"
          else
            echo "‚úÖ ECR repository destroyed"
            ((RESOURCES_DESTROYED++))
          fi
          
          # Check Secrets Manager
          if aws secretsmanager describe-secret --secret-id myapp/db-credentials &>/dev/null; then
            echo "‚ùå Secrets Manager secret still exists"
          else
            echo "‚úÖ Secrets Manager secret destroyed"
            ((RESOURCES_DESTROYED++))
          fi
          
          # Check IAM role
          if aws iam get-role --role-name external-secrets-role &>/dev/null; then
            echo "‚ùå IAM role still exists"
          else
            echo "‚úÖ IAM role destroyed"
            ((RESOURCES_DESTROYED++))
          fi
          
          # Check VPC
          if aws ec2 describe-vpcs --filters "Name=tag:Name,Values=eks-vpc" --query 'Vpcs[0].VpcId' --output text | grep -v "None"; then
            echo "‚ùå VPC still exists"
          else
            echo "‚úÖ VPC destroyed"
            ((RESOURCES_DESTROYED++))
          fi
          
          echo ""
          echo "üéØ DESTRUCTION SUMMARY: $RESOURCES_DESTROYED/$TOTAL_RESOURCES resources destroyed"
          
          if [ $RESOURCES_DESTROYED -eq $TOTAL_RESOURCES ]; then
            echo "üéâ ALL RESOURCES SUCCESSFULLY DESTROYED!"
            echo "üí∞ No more billable resources remain"
          else
            echo "‚ö†Ô∏è Some resources may still exist - check AWS console"
            echo "üí∏ You may still be charged for remaining resources"
          fi# Trigger complete destruction
