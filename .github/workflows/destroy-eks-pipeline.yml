name: EKS Destruction Pipeline

on:
  workflow_dispatch:  # Manual trigger only

env:
  AWS_REGION: us-east-1

jobs:
  terraform-destroy:
    name: Terraform Destroy
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./eks/terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.0.0

      - name: Terraform Init
        run: terraform init -upgrade

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.8.0'
          
      - name: Clean up Kubernetes resources
        run: |
          # Update kubeconfig (may fail if cluster doesn't exist)
          aws eks update-kubeconfig --name my-eks-cluster --region ${{ env.AWS_REGION }} || echo "Cluster may not exist, continuing"
          
          # Only proceed if cluster exists
          if kubectl cluster-info &>/dev/null; then
            echo "Cluster accessible, cleaning up resources..."
            
            # Delete Helm releases
            echo "Deleting Helm releases..."
            helm ls -A -q | xargs -r helm uninstall || echo "No Helm releases to delete"
            
            # Delete external-secrets namespace with timeout
            echo "Deleting external-secrets namespace..."
            kubectl delete namespace external-secrets --ignore-not-found=true --timeout=60s || echo "Namespace deletion timed out, continuing"
            
            # Delete any other application resources with timeout
            echo "Deleting application resources..."
            timeout 60s kubectl delete all --all -n default || echo "Resource deletion timed out or no resources to delete"
            
            echo "Kubernetes cleanup completed"
          else
            echo "Cluster not accessible, skipping Kubernetes cleanup"
          fi

      - name: Get EKS Resources
        id: get-resources
        run: |
          # Get node group name
          NODE_GROUP=$(aws eks list-nodegroups --cluster-name my-eks-cluster --query 'nodegroups[0]' --output text || echo "")
          echo "NODE_GROUP=$NODE_GROUP" >> $GITHUB_OUTPUT
          
          # Get ElastiCache cluster ID
          CACHE_CLUSTER=$(aws elasticache describe-cache-clusters --query 'CacheClusters[?CacheClusterId==`eks-redis`].CacheClusterId' --output text || echo "")
          echo "CACHE_CLUSTER=$CACHE_CLUSTER" >> $GITHUB_OUTPUT
          
          # Get RDS instance ID
          DB_INSTANCE=$(aws rds describe-db-instances --query 'DBInstances[?DBInstanceIdentifier==`eks-postgres`].DBInstanceIdentifier' --output text || echo "")
          echo "DB_INSTANCE=$DB_INSTANCE" >> $GITHUB_OUTPUT
          
          # Get ECR repository name
          ECR_REPO=$(aws ecr describe-repositories --query 'repositories[?repositoryName==`myapp`].repositoryName' --output text || echo "")
          echo "ECR_REPO=$ECR_REPO" >> $GITHUB_OUTPUT

      - name: Delete EKS Node Groups
        run: |
          NODE_GROUP="${{ steps.get-resources.outputs.NODE_GROUP }}"
          if [ "$NODE_GROUP" != "" ] && [ "$NODE_GROUP" != "None" ]; then
            echo "Deleting node group: $NODE_GROUP"
            if aws eks delete-nodegroup --cluster-name my-eks-cluster --nodegroup-name "$NODE_GROUP" 2>/dev/null; then
              echo "Node group deletion initiated"
              # Wait for node group deletion (up to 10 minutes)
              for i in {1..20}; do
                STATUS=$(aws eks describe-nodegroup --cluster-name my-eks-cluster --nodegroup-name "$NODE_GROUP" --query 'nodegroup.status' --output text 2>/dev/null || echo "DELETED")
                echo "Node group status: $STATUS"
                if [ "$STATUS" == "DELETED" ]; then
                  break
                fi
                sleep 30
              done
            else
              echo "Node group already deleted or doesn't exist"
            fi
          else
            echo "No node groups found to delete"
          fi

      - name: Delete ElastiCache Cluster
        run: |
          CACHE_CLUSTER="${{ steps.get-resources.outputs.CACHE_CLUSTER }}"
          if [ "$CACHE_CLUSTER" != "" ] && [ "$CACHE_CLUSTER" != "None" ]; then
            echo "Deleting ElastiCache cluster: $CACHE_CLUSTER"
            if aws elasticache delete-cache-cluster --cache-cluster-id "$CACHE_CLUSTER" 2>/dev/null; then
              echo "ElastiCache deletion initiated"
              # Wait for ElastiCache deletion (up to 10 minutes)
              for i in {1..20}; do
                STATUS=$(aws elasticache describe-cache-clusters --cache-cluster-id "$CACHE_CLUSTER" --query 'CacheClusters[0].CacheClusterStatus' --output text 2>/dev/null || echo "DELETED")
                echo "ElastiCache status: $STATUS"
                if [ "$STATUS" == "DELETED" ]; then
                  break
                fi
                sleep 30
              done
            else
              echo "ElastiCache cluster already deleted or doesn't exist"
            fi
          else
            echo "No ElastiCache clusters found to delete"
          fi

      - name: Delete RDS Instance
        run: |
          DB_INSTANCE="${{ steps.get-resources.outputs.DB_INSTANCE }}"
          if [ "$DB_INSTANCE" != "" ] && [ "$DB_INSTANCE" != "None" ]; then
            echo "Deleting RDS instance: $DB_INSTANCE"
            if aws rds delete-db-instance --db-instance-identifier "$DB_INSTANCE" --skip-final-snapshot 2>/dev/null; then
              echo "RDS deletion initiated"
              # Wait for RDS deletion (up to 15 minutes)
              for i in {1..30}; do
                STATUS=$(aws rds describe-db-instances --db-instance-identifier "$DB_INSTANCE" --query 'DBInstances[0].DBInstanceStatus' --output text 2>/dev/null || echo "DELETED")
                echo "RDS status: $STATUS"
                if [ "$STATUS" == "DELETED" ]; then
                  break
                fi
                sleep 30
              done
            else
              echo "RDS instance already deleted or doesn't exist"
            fi
          else
            echo "No RDS instances found to delete"
          fi
          
      - name: Delete AWS Secrets Manager Secret
        run: |
          echo "Deleting AWS Secrets Manager secret..."
          aws secretsmanager delete-secret --secret-id myapp/db-credentials --force-delete-without-recovery --region ${{ env.AWS_REGION }} || echo "Secret may not exist, continuing"

      - name: Clean up ECR Repository
        run: |
          ECR_REPO="${{ steps.get-resources.outputs.ECR_REPO }}"
          if [ "$ECR_REPO" != "" ] && [ "$ECR_REPO" != "None" ]; then
            echo "Cleaning up ECR repository: $ECR_REPO"
            
            # Delete all images in the repository
            DIGESTS=$(aws ecr list-images --repository-name "$ECR_REPO" --query 'imageIds[*].imageDigest' --output text 2>/dev/null || echo "")
            
            if [ -n "$DIGESTS" ] && [ "$DIGESTS" != "None" ]; then
              echo "Deleting images from ECR repository..."
              for DIGEST in $DIGESTS; do
                echo "Deleting image with digest: $DIGEST"
                aws ecr batch-delete-image --repository-name "$ECR_REPO" --image-ids imageDigest="$DIGEST" 2>/dev/null || echo "Failed to delete image, continuing"
              done
            else
              echo "No images found in repository or repository doesn't exist"
            fi
          else
            echo "No ECR repositories found to clean up"
          fi

      - name: Remove missing resources from state
        run: |
          # Remove resources that don't exist from Terraform state
          echo "Checking and removing missing resources from state..."
          
          # Check and remove Secrets Manager secret if it doesn't exist
          if ! aws secretsmanager describe-secret --secret-id myapp/db-credentials --region us-east-1 &>/dev/null; then
            echo "Secrets Manager secret not found, removing from state"
            terraform state rm aws_secretsmanager_secret.db_credentials || echo "Not in state"
          fi
          
          # Check and remove ElastiCache cluster if it doesn't exist
          if ! aws elasticache describe-cache-clusters --cache-cluster-id eks-redis &>/dev/null; then
            echo "ElastiCache cluster not found, removing from state"
            terraform state rm aws_elasticache_cluster.redis || echo "Not in state"
            terraform state rm aws_elasticache_subnet_group.cache_subnet_group || echo "Not in state"
          fi
          
          # Check and remove RDS instance if it doesn't exist
          if ! aws rds describe-db-instances --db-instance-identifier eks-postgres &>/dev/null; then
            echo "RDS instance not found, removing from state"
            terraform state rm aws_db_instance.postgres || echo "Not in state"
            terraform state rm aws_db_subnet_group.postgres || echo "Not in state"
          fi
          
          # Check and remove ECR repository if it doesn't exist
          if ! aws ecr describe-repositories --repository-names myapp &>/dev/null; then
            echo "ECR repository not found, removing from state"
            terraform state rm aws_ecr_repository.myapp || echo "Not in state"
          fi
          
          # Check and remove EKS cluster if it doesn't exist
          if ! aws eks describe-cluster --name my-eks-cluster &>/dev/null; then
            echo "EKS cluster not found, removing from state"
            terraform state rm 'module.eks.aws_eks_cluster.this[0]' || echo "Not in state"
            terraform state rm 'module.eks.aws_eks_node_group.this["app_nodes"]' || echo "Not in state"
          fi

      - name: Terraform Destroy
        run: |
          echo "Starting Terraform destroy..."
          if ! terraform destroy -auto-approve 2>&1 | tee destroy_output.log; then
            if grep -q "state lock\|ConditionalCheckFailedException" destroy_output.log; then
              echo "‚ùå State lock error - this is an authentic error that must be resolved"
              cat destroy_output.log
              exit 1
            elif grep -q "permission\|forbidden\|unauthorized" destroy_output.log; then
              echo "‚ùå Permission error - this is an authentic error"
              cat destroy_output.log
              exit 1
            elif grep -q "not found\|does not exist\|already deleted\|NoSuchBucket" destroy_output.log; then
              echo "‚úÖ Resources already deleted, continuing"
            else
              echo "‚ùå Terraform destroy failed with authentic error"
              cat destroy_output.log
              exit 1
            fi
          else
            echo "‚úÖ Terraform destroy completed successfully"
          fi
          
      - name: Final cleanup
        run: |
          echo "Performing final cleanup..."
          # Clear any remaining state for missing resources only
          STATE_RESOURCES=$(terraform state list || echo "")
          if [ -n "$STATE_RESOURCES" ]; then
            echo "Removing remaining state resources..."
            echo "$STATE_RESOURCES" | xargs -r -I {} terraform state rm "{}" || echo "State cleanup completed"
          else
            echo "State already clean"
          fi
          echo "üéâ EKS infrastructure destruction process completed"